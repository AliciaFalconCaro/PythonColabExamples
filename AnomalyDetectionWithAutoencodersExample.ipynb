{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmxuqaLO0DjaoOrcLbjvw1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AliciaFalconCaro/PythonColabExamples/blob/main/AnomalyDetectionWithAutoencodersExample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example is based on the tutorial presented in: https://www.geeksforgeeks.org/anomaly-detection-with-tensorflow/"
      ],
      "metadata": {
        "id": "4FQ8Q4jQl3t2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we load the dataset, which is available on the Drive folder. The dataset was obtained from here: https://drive.google.com/file/d/1M4CIY_xH-8ySb615sdFwPvmWVzMLK8uR/view?usp=sharing"
      ],
      "metadata": {
        "id": "TwWNHKKinGSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import pandas as pandas\n",
        "\n",
        "Dataset = pandas.read_csv('creditcard.csv')\n",
        "\n",
        "#let's visualize part of the data\n",
        "#print(Dataset)"
      ],
      "metadata": {
        "id": "u4VMQ12AnPuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We remove (drop) the \"Time\" column as it may not be relevant for anomaly detection\n",
        "Dataset = Dataset.drop(['Time'], axis=1)"
      ],
      "metadata": {
        "id": "2FuR6mPoobnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data cleaning and preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "Dataset['Amount'] = scaler.fit_transform(Dataset['Amount'].values.reshape(-1, 1))\n",
        "Dataset['Class'] = Dataset['Class'].astype(str) # Convert Class column to string for one-hot encoding\n",
        "\n",
        "# Create one-hot encoding for the 'Class' column\n",
        "Dataset = pandas.get_dummies(Dataset, columns=['Class'], prefix=['Class'])"
      ],
      "metadata": {
        "id": "_QuOgNUnoyxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into train and test sets (Train:0.8/Test:0.2)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data, test_data = train_test_split(Dataset, test_size=0.2, random_state=42)\n",
        "\n",
        "# Extract features (X) and labels (y) for training and testing\n",
        "X_train = train_data.drop(['Class_0', 'Class_1'], axis=1).values\n",
        "y_train = train_data[['Class_0', 'Class_1']].values\n",
        "\n",
        "X_test = test_data.drop(['Class_0', 'Class_1'], axis=1).values\n",
        "y_test = test_data[['Class_0', 'Class_1']].values\n"
      ],
      "metadata": {
        "id": "ysdTryAtpqqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Machine Learning Training\n",
        "import tensorflow as tf\n",
        "from keras import layers, models\n",
        "\n",
        "# Build the Autoencoder Model\n",
        "def build_autoencoder(input_shape):\n",
        "\tmodel = models.Sequential()\n",
        "\t# Encoder layer part\n",
        "\tmodel.add(layers.InputLayer(input_shape=input_shape))\n",
        "\tmodel.add(layers.Dense(64, activation='relu'))\n",
        "\tmodel.add(layers.Dense(32, activation='relu'))\n",
        "\tmodel.add(layers.Dense(16, activation='relu')) # bottleneck layer\n",
        "\t# Decoder layer part\n",
        "\tmodel.add(layers.Dense(32, activation='relu'))\n",
        "\tmodel.add(layers.Dense(64, activation='relu'))\n",
        "\tmodel.add(layers.Dense(input_shape, activation='tanh'))\n",
        "\treturn model\n",
        "\n",
        "\n",
        "input_shape = X_train.shape[1]\n",
        "autoencoder = build_autoencoder(input_shape)\n",
        "\n",
        "# Compile the Model\n",
        "autoencoder.compile(optimizer='rmsprop', loss='mse', metrics=['accuracy'])\n",
        "\n",
        "# Train the Autoencoder\n",
        "history = autoencoder.fit(X_train, X_train, epochs=25, batch_size=64, shuffle=False, validation_data=(X_test, X_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKlNE1sEqLpF",
        "outputId": "e3de4a22-bde8-4a45-c926-b9a922eebfc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "3561/3561 [==============================] - 13s 3ms/step - loss: 0.5415 - accuracy: 0.6403 - val_loss: 0.4628 - val_accuracy: 0.7117\n",
            "Epoch 2/25\n",
            "3561/3561 [==============================] - 11s 3ms/step - loss: 0.4701 - accuracy: 0.7407 - val_loss: 0.4513 - val_accuracy: 0.7467\n",
            "Epoch 3/25\n",
            "3561/3561 [==============================] - 11s 3ms/step - loss: 0.4608 - accuracy: 0.7557 - val_loss: 0.4464 - val_accuracy: 0.7374\n",
            "Epoch 4/25\n",
            "3561/3561 [==============================] - 9s 3ms/step - loss: 0.4565 - accuracy: 0.7634 - val_loss: 0.4429 - val_accuracy: 0.7556\n",
            "Epoch 5/25\n",
            "3561/3561 [==============================] - 14s 4ms/step - loss: 0.4539 - accuracy: 0.7670 - val_loss: 0.4371 - val_accuracy: 0.7377\n",
            "Epoch 6/25\n",
            "3561/3561 [==============================] - 12s 3ms/step - loss: 0.4522 - accuracy: 0.7713 - val_loss: 0.4379 - val_accuracy: 0.7456\n",
            "Epoch 7/25\n",
            "3561/3561 [==============================] - 13s 4ms/step - loss: 0.4509 - accuracy: 0.7719 - val_loss: 0.4356 - val_accuracy: 0.7575\n",
            "Epoch 8/25\n",
            "3561/3561 [==============================] - 10s 3ms/step - loss: 0.4498 - accuracy: 0.7727 - val_loss: 0.4369 - val_accuracy: 0.7599\n",
            "Epoch 9/25\n",
            "3561/3561 [==============================] - 10s 3ms/step - loss: 0.4488 - accuracy: 0.7733 - val_loss: 0.4330 - val_accuracy: 0.7745\n",
            "Epoch 10/25\n",
            "3561/3561 [==============================] - 11s 3ms/step - loss: 0.4480 - accuracy: 0.7748 - val_loss: 0.4327 - val_accuracy: 0.7649\n",
            "Epoch 11/25\n",
            "3561/3561 [==============================] - 10s 3ms/step - loss: 0.4474 - accuracy: 0.7746 - val_loss: 0.4324 - val_accuracy: 0.7691\n",
            "Epoch 12/25\n",
            "3561/3561 [==============================] - 9s 3ms/step - loss: 0.4470 - accuracy: 0.7743 - val_loss: 0.4328 - val_accuracy: 0.7692\n",
            "Epoch 13/25\n",
            "3561/3561 [==============================] - 10s 3ms/step - loss: 0.4466 - accuracy: 0.7753 - val_loss: 0.4315 - val_accuracy: 0.7722\n",
            "Epoch 14/25\n",
            "3561/3561 [==============================] - 11s 3ms/step - loss: 0.4462 - accuracy: 0.7746 - val_loss: 0.4307 - val_accuracy: 0.7702\n",
            "Epoch 15/25\n",
            "3561/3561 [==============================] - 9s 2ms/step - loss: 0.4460 - accuracy: 0.7739 - val_loss: 0.4321 - val_accuracy: 0.7734\n",
            "Epoch 16/25\n",
            "3561/3561 [==============================] - 11s 3ms/step - loss: 0.4457 - accuracy: 0.7729 - val_loss: 0.4316 - val_accuracy: 0.7769\n",
            "Epoch 17/25\n",
            "3561/3561 [==============================] - 11s 3ms/step - loss: 0.4448 - accuracy: 0.7730 - val_loss: 0.4277 - val_accuracy: 0.7783\n",
            "Epoch 18/25\n",
            "3561/3561 [==============================] - 10s 3ms/step - loss: 0.4442 - accuracy: 0.7735 - val_loss: 0.4277 - val_accuracy: 0.7909\n",
            "Epoch 19/25\n",
            "3561/3561 [==============================] - 9s 3ms/step - loss: 0.4439 - accuracy: 0.7726 - val_loss: 0.4284 - val_accuracy: 0.7810\n",
            "Epoch 20/25\n",
            "3561/3561 [==============================] - 11s 3ms/step - loss: 0.4436 - accuracy: 0.7709 - val_loss: 0.4300 - val_accuracy: 0.7780\n",
            "Epoch 21/25\n",
            "3561/3561 [==============================] - 10s 3ms/step - loss: 0.4434 - accuracy: 0.7705 - val_loss: 0.4284 - val_accuracy: 0.7708\n",
            "Epoch 22/25\n",
            "3561/3561 [==============================] - 9s 3ms/step - loss: 0.4431 - accuracy: 0.7705 - val_loss: 0.4281 - val_accuracy: 0.7685\n",
            "Epoch 23/25\n",
            "3561/3561 [==============================] - 11s 3ms/step - loss: 0.4429 - accuracy: 0.7697 - val_loss: 0.4284 - val_accuracy: 0.7755\n",
            "Epoch 24/25\n",
            "3561/3561 [==============================] - 11s 3ms/step - loss: 0.4426 - accuracy: 0.7699 - val_loss: 0.4275 - val_accuracy: 0.7688\n",
            "Epoch 25/25\n",
            "3561/3561 [==============================] - 9s 3ms/step - loss: 0.4424 - accuracy: 0.7700 - val_loss: 0.4292 - val_accuracy: 0.7773\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Evaluation\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Evaluate the Autoencoder\n",
        "predictions = autoencoder.predict(X_test)\n",
        "mse = np.mean(np.power(X_test - predictions, 2), axis=1)\n",
        "\n",
        "# Set a threshold for anomaly detection\n",
        "threshold = 0.6\n",
        "\n",
        "# Classify anomalies based on the threshold\n",
        "anomalies = mse > threshold\n",
        "\n",
        "# Evaluate the Anomaly Detection Model\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "y_pred = anomalies.astype(int)\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f'Test Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "0tBSJazBqnf7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}